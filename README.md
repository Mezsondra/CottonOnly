# ğŸ§µ 100% Cotton Clothing Scraper

A Python-based web scraper that finds **100% cotton** clothing products from multiple retailers across UK and USA.

## âœ¨ Features

- **Multi-retailer support**: H&M, ASOS, Uniqlo, Gap, Next, M&S, and more
- **Multi-region**: UK and USA with region-specific pricing
- **Gender filtering**: Men, Women, Kids
- **Material verification**: Only scrapes products confirmed to be 100% cotton
- **Async scraping**: Fast parallel processing with Playwright
- **Extensible**: Easy to add new retailers

## ğŸ“ Project Structure

```
cotton-scraper/
â”œâ”€â”€ main.py                 # Main entry point
â”œâ”€â”€ config.py               # Configuration settings
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py     # Abstract base class
â”‚   â”œâ”€â”€ hm_scraper.py       # H&M scraper
â”‚   â”œâ”€â”€ asos_scraper.py     # ASOS scraper
â”‚   â””â”€â”€ generic_scraper.py  # Generic/configurable scraper
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ helpers.py          # Utility functions
â””â”€â”€ data/                   # Scraped data output (JSON)
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone or download the project
cd cotton-scraper

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium
```

### Web Interface (Recommended) ğŸŒ

The easiest way to use the scraper is through the web interface:

```bash
# Start the web application
python app.py

# Open your browser to:
# http://localhost:5000
```

**Features:**
- ğŸ¨ Beautiful, responsive UI
- ğŸ“Š Real-time scraping progress
- ğŸ” Search and filter products
- ğŸ“¥ Download results as JSON
- ğŸ“ˆ View scraping history
- ğŸš€ Multi-retailer concurrent scraping

### Command Line Usage

```bash
# Run demo (quick test with H&M UK)
python main.py --demo

# Scrape all retailers in UK
python main.py --region UK

# Scrape USA retailers
python main.py --region USA

# Scrape specific retailer
python main.py --retailer hm

# Scrape specific genders
python main.py --gender men women

# Scrape multiple retailers concurrently (faster!)
python main.py --region UK --retailer hm asos uniqlo --concurrent

# Combine options
python main.py --region USA --retailer hm asos --gender men

# Show all configured retailers
python show_retailers.py
```

### Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--region` / `-r` | Region to scrape (UK, USA, ALL) | UK |
| `--retailer` / `-s` | Specific retailer(s) | All available |
| `--gender` / `-g` | Gender categories | All (men, women, kids) |
| `--output` / `-o` | Output filename | all_products.json |
| `--demo` | Quick demo mode | False |
| `--concurrent` | Scrape multiple retailers in parallel | False |

## ğŸ“Š Output Format

Products are saved as JSON with the following structure:

```json
{
  "scraped_at": "2024-01-15T10:30:00",
  "total_products": 150,
  "products": [
    {
      "id": "abc123def456",
      "name": "Pure Cotton T-Shirt",
      "brand": "H&M",
      "price": 12.99,
      "currency": "GBP",
      "url": "https://www2.hm.com/...",
      "image_url": "https://...",
      "gender": "men",
      "category": "t-shirts",
      "material": "100% Cotton",
      "color": "White",
      "sizes": ["S", "M", "L", "XL"],
      "source": "hm",
      "region": "UK",
      "scraped_at": "2024-01-15T10:30:00"
    }
  ]
}
```

## ğŸŒ Supported Retailers

### UK Region (9 retailers)
| Retailer | Status | Notes |
|----------|--------|-------|
| H&M | âœ… Active | Full support with dedicated scraper |
| ASOS | âœ… Active | Dedicated scraper with material filter |
| Uniqlo | ğŸ”§ Generic | Configurable generic scraper |
| Next | ğŸ”§ Generic | Configurable generic scraper |
| Marks & Spencer | ğŸ”§ Generic | Configurable generic scraper |
| Zara | ğŸ”§ Generic | Configurable generic scraper |
| Primark | ğŸ”§ Generic | Configurable generic scraper |
| John Lewis | ğŸ”§ Generic | Configurable generic scraper |
| Debenhams | ğŸ”§ Generic | Configurable generic scraper |

### USA Region (10 retailers)
| Retailer | Status | Notes |
|----------|--------|-------|
| H&M | âœ… Active | Full support with dedicated scraper |
| Uniqlo | ğŸ”§ Generic | Configurable generic scraper |
| Gap | ğŸ”§ Generic | Configurable generic scraper |
| Old Navy | ğŸ”§ Generic | Configurable generic scraper |
| Target | ğŸ”§ Generic | Configurable generic scraper |
| Zara | ğŸ”§ Generic | Configurable generic scraper |
| Macy's | ğŸ”§ Generic | Configurable generic scraper |
| Nordstrom | ğŸ”§ Generic | Configurable generic scraper |
| J.Crew | ğŸ”§ Generic | Configurable generic scraper |
| Banana Republic | ğŸ”§ Generic | Configurable generic scraper |

## ğŸ”§ Adding New Retailers

### Option 1: Use Generic Scraper (Easy)

Add configuration to `config.py`:

```python
RETAILERS["newstore"] = {
    "name": "New Store",
    "base_urls": {
        "UK": "https://www.newstore.co.uk",
        "USA": "https://www.newstore.com"
    },
    "search_paths": {
        "men": "/mens-clothing",
        "women": "/womens-clothing",
        "kids": "/kids-clothing"
    },
    "supports_material_filter": False
}
```

### Option 2: Create Custom Scraper (Advanced)

Create a new file `scrapers/newstore_scraper.py`:

```python
from scrapers.base_scraper import BaseScraper

class NewStoreScraper(BaseScraper):
    @property
    def retailer_name(self) -> str:
        return "New Store"
    
    @property
    def retailer_id(self) -> str:
        return "newstore"
    
    async def get_base_url(self) -> str:
        # Implementation
        pass
    
    async def scrape_category(self, gender, page):
        # Implementation
        pass
    
    async def get_product_details(self, url, page):
        # Implementation
        pass
```

## ğŸ” Material Detection

The scraper looks for these patterns to identify 100% cotton:

- `100% cotton`
- `100% organic cotton`
- `100% BCI cotton`
- `pure cotton`
- `all cotton`

Pattern matching is case-insensitive and handles various formatting.

## âš¡ Performance Features

### Concurrent Scraping
The scraper now supports parallel execution for multiple retailers:

```bash
# Scrape multiple retailers at the same time (faster)
python main.py --region UK --retailer hm asos uniqlo zara --concurrent
```

**Benefits:**
- Significantly faster when scraping multiple retailers
- Each retailer runs in parallel with its own browser instance
- Ideal for scraping all retailers in a region

**Note:** Concurrent mode uses more system resources (CPU, memory, network).

## âš ï¸ Important Notes

### Rate Limiting
- Default delay between requests: 2-3.5 seconds
- Respects website rate limits
- Use `--demo` mode for testing
- Concurrent mode scrapes retailers in parallel but still respects per-retailer delays

### Legal Considerations
- Only scrape public product information
- Respect robots.txt directives
- Don't overload servers
- Use data responsibly

### Website Changes
Websites frequently update their structure. If scraping fails:

1. Check if the website is accessible
2. Inspect the page structure for changes
3. Update selectors in the scraper
4. Consider using the generic scraper as fallback

## ğŸ—„ï¸ Database Integration (Future)

The output JSON can be imported into a database:

```python
# Example: PostgreSQL with SQLAlchemy
from sqlalchemy import create_engine
import json

with open('data/all_products.json') as f:
    data = json.load(f)

# Insert into database
for product in data['products']:
    # Insert logic here
    pass
```

## ğŸ” Utility Scripts

### Show Retailers
View all configured retailers and their availability:

```bash
# Show all retailers
python show_retailers.py

# Show details for a specific retailer
python show_retailers.py hm
```

This will display:
- All configured retailers grouped by region
- Base URLs for each region
- Current availability status
- Total retailer count

## ğŸ“ˆ Roadmap

- [x] Add more retailers (Zara, Primark, John Lewis, etc.)
- [x] Concurrent/parallel scraping support
- [x] Enhanced generic scraper for broader compatibility
- [ ] Scheduled scraping with cron
- [ ] Price change tracking
- [ ] Database integration (PostgreSQL)
- [ ] API endpoint for the scraped data
- [ ] Docker containerization

## ğŸ¤ Contributing

To add support for a new retailer:

1. Test the website structure manually
2. Create a scraper (generic or custom)
3. Add configuration to `config.py`
4. Test thoroughly with `--demo` mode
5. Submit changes

## ğŸ“œ License

MIT License - Use freely for personal and commercial projects.

---

Made with ğŸ§µ for cotton lovers everywhere!
